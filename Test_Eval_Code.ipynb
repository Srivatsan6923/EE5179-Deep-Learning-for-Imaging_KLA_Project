{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dhkIys-m5jG",
        "outputId": "e4d057e5-406a-4e10-8b0a-4eb8e29ac0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EE5179-Deep-Learning-for-Imaging_KLA_Project'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 83 (delta 45), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (83/83), 15.09 MiB | 19.01 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n",
            "/content/EE5179-Deep-Learning-for-Imaging_KLA_Project/EE5179-Deep-Learning-for-Imaging_KLA_Project\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Tarang-Mendhe/EE5179-Deep-Learning-for-Imaging_KLA_Project\n",
        "%cd EE5179-Deep-Learning-for-Imaging_KLA_Project\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "'''\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "# from 'model_folder' import model\n",
        "from best_model_arch import UNet\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def calculate_psnr( prediction,target,mask,only_defect):\n",
        "\n",
        "\n",
        "    if only_defect:\n",
        "        mask = mask.to(torch.bool)\n",
        "        mask_target = target[mask]\n",
        "        mask_prediction = prediction[mask]\n",
        "    else:\n",
        "        mask_prediction=prediction\n",
        "        mask_target=target\n",
        "    # print(\"b\",mask_prediction.shape)\n",
        "\n",
        "    mse = F.mse_loss(mask_prediction, mask_target)\n",
        "\n",
        "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))  # Normalizing by 1.0 since images are in [0, 1] range\n",
        "    return psnr.item()\n",
        "\n",
        "class SSIM(nn.Module):\n",
        "    \"\"\"Layer to compute the SSIM loss between a pair of images\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
        "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
        "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
        "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
        "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
        "\n",
        "        self.refl = nn.ReflectionPad2d(1)\n",
        "\n",
        "        self.C1 = 0.01 ** 2\n",
        "        self.C2 = 0.03 ** 2\n",
        "\n",
        "    def forward(self, prediction,target,mask,only_defect):\n",
        "\n",
        "        x = self.refl(target)\n",
        "        y = self.refl(prediction)\n",
        "\n",
        "        mu_x = self.mu_x_pool(x)\n",
        "        mu_y = self.mu_y_pool(y)\n",
        "\n",
        "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
        "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
        "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
        "\n",
        "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
        "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
        "        if only_defect:\n",
        "         return (torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1))*mask\n",
        "        else:\n",
        "            return (torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1))\n",
        "\n",
        "\n",
        "\n",
        "class DenoisingDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Collect image paths in organized structure\n",
        "        self._collect_image_paths()\n",
        "\n",
        "    def _collect_image_paths(self):\n",
        "\n",
        "        for category in os.listdir(self.root_dir):\n",
        "            category_path = os.path.join(self.root_dir, category)\n",
        "\n",
        "            if not os.path.isdir(category_path):\n",
        "                continue\n",
        "\n",
        "            for dataset_type in ['Test']:\n",
        "                dataset_dir = os.path.join(category_path, dataset_type)\n",
        "\n",
        "                if not os.path.isdir(dataset_dir):\n",
        "                    continue\n",
        "\n",
        "                gt_clean_dir = os.path.join(dataset_dir, 'Clean')\n",
        "                defect_mask_dir = os.path.join(dataset_dir, 'GT_mask')\n",
        "                degraded_image_dir = os.path.join(dataset_dir, 'Noise')\n",
        "\n",
        "                # Go through each type subfolder (e.g., \"type1\", \"type2\")\n",
        "                for type_subfolder in os.listdir(gt_clean_dir):\n",
        "                    type_clean_dir = os.path.join(gt_clean_dir, type_subfolder)\n",
        "                    type_defect_mask_dir = os.path.join(defect_mask_dir, type_subfolder)\n",
        "                    type_degraded_dir = os.path.join(degraded_image_dir, type_subfolder)\n",
        "\n",
        "                    if not (os.path.isdir(type_clean_dir) and os.path.isdir(type_defect_mask_dir) and os.path.isdir(type_degraded_dir)):\n",
        "                        continue\n",
        "\n",
        "                    # Collect images within each type folder\n",
        "                    for file in os.listdir(type_clean_dir):\n",
        "                        if file.endswith('.png') or file.endswith('.jpg'):\n",
        "                            # Paths for GT_clean, defect mask, and degraded image\n",
        "                            gt_clean_path = os.path.join(type_clean_dir, file)\n",
        "                            base_name, _ = os.path.splitext(file)\n",
        "\n",
        "                            # Construct corresponding defect mask and degraded image paths\n",
        "                            defect_mask_file = f\"{base_name}_mask.png\"\n",
        "                            defect_mask_path = os.path.join(type_defect_mask_dir, defect_mask_file)\n",
        "\n",
        "                            degraded_image_path = os.path.join(type_degraded_dir, file)\n",
        "\n",
        "                            # Add to data list only if all paths exist\n",
        "                            if os.path.exists(defect_mask_path) and os.path.exists(degraded_image_path):\n",
        "                                self.data.append({\n",
        "                                    'file_name':file,\n",
        "                                    'categories':category,\n",
        "                                    'gt_clean': gt_clean_path,\n",
        "                                    'defect_mask': defect_mask_path,\n",
        "                                    'degraded_image': degraded_image_path\n",
        "                                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the GT_clean_image, Defect_mask, and Degraded_image for the given index\n",
        "        \"\"\"\n",
        "        paths = self.data[idx]\n",
        "        filenames=paths['file_name']\n",
        "        catg=paths['categories']\n",
        "        gt_clean_img = cv2.imread(paths['gt_clean'])\n",
        "        defect_mask_img = cv2.imread(paths['defect_mask'])\n",
        "        degraded_img = cv2.imread(paths['degraded_image'])\n",
        "\n",
        "        # Convert images to RGB\n",
        "        gt_clean_img = cv2.cvtColor(gt_clean_img, cv2.COLOR_BGR2RGB)\n",
        "        defect_mask_img = cv2.cvtColor(defect_mask_img, cv2.COLOR_BGR2RGB)\n",
        "        degraded_img = cv2.cvtColor(degraded_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        degraded_img = cv2.resize(degraded_img, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "        gt_clean_img = cv2.resize(gt_clean_img, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "        defect_mask_img = cv2.resize(defect_mask_img, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            gt_clean_img = self.transform(gt_clean_img)\n",
        "            defect_mask_img = self.transform(defect_mask_img)\n",
        "            degraded_img = self.transform(degraded_img)\n",
        "        else:\n",
        "            # Default to converting to PyTorch tensors\n",
        "            gt_clean_img = torch.tensor(gt_clean_img).permute(2, 0, 1).float() / 255.0\n",
        "            defect_mask_img = torch.tensor(defect_mask_img).permute(2, 0, 1).float() / 255.0\n",
        "            degraded_img = torch.tensor(degraded_img).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return {\n",
        "            'file_name':filenames,\n",
        "            'categories':catg,\n",
        "            'gt_clean': gt_clean_img,\n",
        "            'defect_mask': defect_mask_img,\n",
        "            'degraded_image': degraded_img\n",
        "        }\n",
        "\n",
        "\n",
        "dataset_root = \"/content/drive/MyDrive/Project_evaluation_6th_Nov/Denoising_Dataset_Test_PSNR_SSIM\" # Give root directory of your dataset\n",
        "\n",
        "\n",
        "transform = None  # Add any custom transformations if needed\n",
        "\n",
        "output_dir= \"/content/drive/MyDrive/Project_evaluation_6th_Nov/Outputs\"  ##### GIVE OUTPUT DIRECTORY PATH for SAVING IMAGES ###############\n",
        "\n",
        "## ######################### instantiate your model and load the weights HERE  #########\n",
        "\n",
        "model_weights_path = \"/content/drive/MyDrive/best_model.pth\"           # location in collab , where model is present\n",
        "## ######################### instantiate your model and load the weights HERE  #########\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_classes = 3  # Specify the number of classes for segmentation\n",
        "model = UNet(n_class =n_classes)\n",
        "model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = DenoisingDataset(dataset_root, transform=transform)\n",
        "\n",
        "# Create a DataLoader for the dataset\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "psnr_values = {}\n",
        "ssim_values = {}\n",
        "calculate_ssim=SSIM()\n",
        "\n",
        "count=0\n",
        "for batch in dataloader:\n",
        "    filename=batch['file_name'][0]\n",
        "    category=batch['categories'][0]\n",
        "    gt_clean_batch = batch['gt_clean']\n",
        "    defect_mask_batch = batch['defect_mask']\n",
        "    degraded_image_batch = batch['degraded_image']\n",
        "    count=count+1\n",
        "    # print(filename)\n",
        "\n",
        "      #####  WRITE CODE TO PASS THE INPUT TO THE MODEL #######\n",
        "    with torch.no_grad():\n",
        "\n",
        "\n",
        "        predicted = model(degraded_image_batch)\n",
        "\n",
        "        #for resizing\n",
        "        predicted_resized = F.interpolate(predicted, size=gt_clean_batch.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "    ### CHANGE 'only_defect' TO 'True' FOR CALCULATING METRIC FOR DEFECT REGION ONLY\n",
        "    psnr_value = calculate_psnr(predicted_resized, gt_clean_batch, defect_mask_batch,only_defect=False)\n",
        "    psnr_values[category] = psnr_values.get(category, []) + [psnr_value]\n",
        "\n",
        "    ssim_value=calculate_ssim(predicted_resized, gt_clean_batch, defect_mask_batch,only_defect=False)\n",
        "\n",
        "\n",
        "    ssim_value = 1 - (ssim_value).mean()\n",
        "\n",
        "    ssim_values[category] = ssim_values.get(category, []) + [ssim_value]\n",
        "\n",
        "    #### SAVING IMAGES #######################\n",
        "    if count%5==0:\n",
        "        save_path=os.path.join(output_dir,f\"{category}_{count}_{filename}\")\n",
        "\n",
        "\n",
        "        concatenated_image=torch.cat([degraded_image_batch[0],predicted_resized[0],gt_clean_batch[0]],dim=2)\n",
        "\n",
        "\n",
        "        save_image( concatenated_image,save_path)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate averages per category and total averages\n",
        "avg_psnr = {category: float(np.mean(psnr)) for category, psnr in psnr_values.items()}\n",
        "mean_ssim = {category: float(np.mean(ssim)) for category, ssim in ssim_values.items()}\n",
        "print(\"average_psnr_catg ===\", avg_psnr)\n",
        "print(\"average_SSIM_catg ===\", mean_ssim)\n",
        "\n",
        "Total_avg_psnr = float(sum(avg_psnr.values()) / len(avg_psnr))\n",
        "Total_avg_ssim = float(sum(mean_ssim.values()) / len(mean_ssim))\n",
        "print(\"TOTAL_AVG_PSNR ===\", Total_avg_psnr)\n",
        "print(\"TOTAL_AVG_SSIM ===\", Total_avg_ssim)\n",
        "\n",
        "# Organize data in a single dictionary for JSON serialization\n",
        "results1 = {\n",
        "    \"average_psnr_per_category\": avg_psnr,\n",
        "    \"average_ssim_per_category\": mean_ssim,\n",
        "    \"total_average_psnr\": Total_avg_psnr,\n",
        "    \"total_average_ssim\": Total_avg_ssim\n",
        "}\n",
        "\n",
        "# Write the results to a JSON file with indentation for readability\n",
        "with open('Avg_metrics.json', 'w') as fp:\n",
        "    json.dump(results1, fp, indent=4)\n",
        "\n",
        "print(\"Results saved to Avg_metrics.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VVmwFKYnJ4N",
        "outputId": "f25e9466-2780-4c8d-e3a4-9069fee01add"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-1f0a4c73cb9d>:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_weights_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average_psnr_catg === {'metal_nut': 18.632296562194824, 'zipper': 13.992872047424317, 'bottle': 11.948722145774148, 'pill': 15.029626039358286, 'carpet': 19.23450756072998, 'hazelnut': 20.919924302534625, 'grid': 18.025960710313583, 'toothbrush': 15.769021987915039, 'wood': 18.66476567586263, 'cable': 16.48510316212972, 'tile': 19.693086079188756, 'capsule': 12.411510467529297, 'transistor': 19.615981783185685, 'leather': 25.82487201690674, 'screw': 12.62409496307373}\n",
            "average_SSIM_catg === {'metal_nut': 0.8218196630477905, 'zipper': 0.7950751185417175, 'bottle': 0.8794946670532227, 'pill': 0.819138765335083, 'carpet': 0.6402596235275269, 'hazelnut': 0.9226533770561218, 'grid': 0.7801556587219238, 'toothbrush': 0.7585717439651489, 'wood': 0.8152282238006592, 'cable': 0.857848048210144, 'tile': 0.7807979583740234, 'capsule': 0.9227038621902466, 'transistor': 0.8854418992996216, 'leather': 0.8002327680587769, 'screw': 0.9293556213378906}\n",
            "TOTAL_AVG_PSNR === 17.258156366941424\n",
            "TOTAL_AVG_SSIM === 0.8272517999013265\n",
            "Results saved to Avg_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from 'model_folder' import model\n",
        "from best_model_arch import UNet\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Root directory containing the 'Noise' and 'Clean' subdirectories.\n",
        "            transform (callable, optional): Optional transform to apply to both images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.noise_dir = os.path.join(root_dir, 'Noise')\n",
        "        self.gt_dir = os.path.join(root_dir, 'Clean')\n",
        "        self.transform = transform\n",
        "\n",
        "        # List of all image files in the Noise directory\n",
        "        self.image_files = sorted(os.listdir(self.noise_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the filename and construct the paths for both Noise and GT images\n",
        "        filename = self.image_files[idx]\n",
        "        noise_path = os.path.join(self.noise_dir, filename)\n",
        "        gt_path = os.path.join(self.gt_dir, filename)\n",
        "\n",
        "        # Load images\n",
        "        noise_image = cv2.imread(noise_path)\n",
        "        gt_image = cv2.imread(gt_path)\n",
        "\n",
        "        # Convert BGR (OpenCV format) to RGB\n",
        "        noise_image = cv2.cvtColor(noise_image, cv2.COLOR_BGR2RGB)\n",
        "        gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        noise_image = cv2.resize(noise_image, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "        gt_image = cv2.resize(gt_image, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "        #defect_mask_img = cv2.resize(defect_mask_img, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            noise_image = self.transform(noise_image)\n",
        "            gt_image = self.transform(gt_image)\n",
        "        else:\n",
        "            # Default transformation: Convert to PyTorch tensors and normalize to [0,1]\n",
        "            noise_image = torch.tensor(noise_image).permute(2, 0, 1).float() / 255.0\n",
        "            gt_image = torch.tensor(gt_image).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        return {'noise': noise_image, 'gt': gt_image, 'filename': filename}\n",
        "\n",
        "\n",
        "dataset_root = \"/content/drive/MyDrive/Project_evaluation_6th_Nov/Denoising_Dataset_Test_Visual\"  # Update this with the actual path to your data directory\n",
        "\n",
        "output_dir=\"/content/drive/MyDrive/Project_evaluation_6th_Nov/Denoising_Dataset_Test_Visual/Output\"  ##### OUTPUT DIRECTORY  ###############\n",
        "\n",
        "transform = transforms.ToTensor()  # Define any additional transformations if needed\n",
        "\n",
        "####  CALL YOUR MODEL\n",
        "\n",
        "model_weights_path = \"/content/drive/MyDrive/best_model.pth\"           # location in collab , where model is present\n",
        "## ######################### instantiate your model and load the weights HERE  #########\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_classes = 3  # Specify the number of classes for segmentation\n",
        "model = UNet(n_class =n_classes)\n",
        "model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset = PairedImageDataset(root_dir=dataset_root, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "for batch in dataloader:\n",
        "    noise_images = batch['noise']\n",
        "    gt_images = batch['gt']\n",
        "    filenames = batch['filename']\n",
        "\n",
        "    # predicted= ## WRITE CODE TO PASS INPUT TO THE MODEL\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predicted = model(noise_images)\n",
        "\n",
        "\n",
        "    save_path=os.path.join(output_dir,filenames[0])\n",
        "    concatenated_image=torch.cat([noise_images[0],predicted[0],gt_images[0]],dim=2)\n",
        "    save_image( concatenated_image,save_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl25eEaCzTeJ",
        "outputId": "f2bd522b-e7e3-45aa-b887-7ddd1cd9d106"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-59f72cc14848>:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_weights_path, map_location=device))\n"
          ]
        }
      ]
    }
  ]
}