{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSVRcBetXiyX"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sut_5N2cWtMl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Define paths\n",
        "zip_file_path = '/content/drive/MyDrive/Denoising_Dataset_train_val.zip'\n",
        "extract_path = '/content/Denoising_Dataset_train_val'\n",
        "\n",
        "# Step 1: Extract the main zip file (if not already extracted)\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# Step 2: Custom Dataset to handle nested folder structure and mask naming convention\n",
        "class DenoisingDataset(Dataset):\n",
        "    def __init__(self, root_dir, data_type='Train', transform=None):\n",
        "        \"\"\"\n",
        "        root_dir : Directory where the dataset is located\n",
        "        data_type : 'Train' or 'Val' to specify which set to load\n",
        "        transform : Any transformations to be applied on the images\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.data_type = data_type\n",
        "        self.transform = transform\n",
        "\n",
        "        # Prepare lists to hold the file paths of clean, degraded, and mask images\n",
        "        self.clean_images = []\n",
        "        self.degraded_images = []\n",
        "        self.mask_images = []\n",
        "\n",
        "        # Step 3: Traverse through the nested folder structure\n",
        "        for category in os.listdir(root_dir):  # First level (e.g., 'bottle', 'cable')\n",
        "            category_path = os.path.join(root_dir, category, self.data_type)  # Choose Train or Val\n",
        "\n",
        "            gt_clean_image_path = os.path.join(category_path, 'GT_clean_image')\n",
        "            degraded_image_path = os.path.join(category_path, 'Degraded_image')\n",
        "            defect_mask_path = os.path.join(category_path, 'Defect_mask')\n",
        "\n",
        "            # Subfolders within 'GT_clean_image', 'Degraded_image', 'Defect_mask' (e.g., 'broken_large', 'broken_small')\n",
        "            for subfolder in os.listdir(gt_clean_image_path):\n",
        "                clean_subfolder = os.path.join(gt_clean_image_path, subfolder)\n",
        "                degraded_subfolder = os.path.join(degraded_image_path, subfolder)\n",
        "                mask_subfolder = os.path.join(defect_mask_path, subfolder)\n",
        "\n",
        "                # For each image in the subfolder, gather the corresponding file paths\n",
        "                for img_name in os.listdir(clean_subfolder):\n",
        "                    clean_img = os.path.join(clean_subfolder, img_name)\n",
        "                    degraded_img = os.path.join(degraded_subfolder, img_name)\n",
        "\n",
        "                    # Construct the corresponding mask file name (e.g., '000.png' -> '000_mask.png')\n",
        "                    img_base_name = os.path.splitext(img_name)[0]  # Get base name without extension\n",
        "                    mask_img = os.path.join(mask_subfolder, f'{img_base_name}_mask.png')\n",
        "\n",
        "                    # Append to lists\n",
        "                    self.clean_images.append(clean_img)\n",
        "                    self.degraded_images.append(degraded_img)\n",
        "                    self.mask_images.append(mask_img)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_img = Image.open(self.clean_images[idx]).convert('RGB')\n",
        "        degraded_img = Image.open(self.degraded_images[idx]).convert('RGB')\n",
        "        mask_img = Image.open(self.mask_images[idx]).convert('L')  # Mask as grayscale\n",
        "\n",
        "        if self.transform:\n",
        "            clean_img = self.transform(clean_img)\n",
        "            degraded_img = self.transform(degraded_img)\n",
        "            mask_img = self.transform(mask_img)\n",
        "\n",
        "        return clean_img, degraded_img, mask_img\n",
        "\n",
        "# Step 4: Define transformations (resizing to a uniform size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize all images to the same size\n",
        "    transforms.ToTensor()           # Convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Step 5: Load training and validation datasets\n",
        "data_path = os.path.join(extract_path, 'Denoising_Dataset_train_val')\n",
        "train_dataset = DenoisingDataset(root_dir=data_path, data_type='Train', transform=transform)\n",
        "val_dataset = DenoisingDataset(root_dir=data_path, data_type='Val', transform=transform)\n",
        "\n",
        "print(f\"Number of images in the training set: {len(train_dataset)}\")\n",
        "print(f\"Number of images in the validation set: {len(val_dataset)}\")\n",
        "\n",
        "# Step 6: Create DataLoader for training and validation datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Step 7: Function to plot random images (clean, degraded, mask)\n",
        "def plot_random_images(dataset, n_images=5):\n",
        "    fig, axes = plt.subplots(n_images, 3, figsize=(15, 15))  # 3 columns for clean, degraded, mask\n",
        "    for i in range(n_images):\n",
        "        clean_img, degraded_img, mask_img = random.choice(dataset)\n",
        "        clean_img_np = clean_img.permute(1, 2, 0).numpy()\n",
        "        degraded_img_np = degraded_img.permute(1, 2, 0).numpy()\n",
        "        mask_img_np = mask_img.squeeze(0).numpy()  # Grayscale, so remove channel dimension\n",
        "\n",
        "        axes[i, 0].imshow(clean_img_np)\n",
        "        axes[i, 0].set_title('Clean Image')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(degraded_img_np)\n",
        "        axes[i, 1].set_title('Degraded Image')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        axes[i, 2].imshow(mask_img_np, cmap='gray')\n",
        "        axes[i, 2].set_title('Mask Image')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot random images from the train and validation datasets\n",
        "print(\"Random images from the training set:\")\n",
        "plot_random_images(train_dataset)\n",
        "\n",
        "print(\"Random images from the validation set:\")\n",
        "plot_random_images(val_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1zNaChaXIHE"
      },
      "source": [
        "# Training and Validation Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvq-gDbCXGqJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DenoisingDataset(Dataset):\n",
        "    def __init__(self, root_dir, data_type='Train', transform=None):\n",
        "        \"\"\"\n",
        "        root_dir : Directory where the dataset is located\n",
        "        data_type : 'Train' or 'Val' to specify which set to load\n",
        "        transform : Any transformations to be applied on the images\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.data_type = data_type\n",
        "        self.transform = transform\n",
        "\n",
        "        # Prepare lists to hold the file paths of clean and degraded images\n",
        "        self.clean_images = []\n",
        "        self.degraded_images = []\n",
        "\n",
        "        # Step 3: Traverse through the nested folder structure\n",
        "        for category in os.listdir(root_dir):  # First level (e.g., 'bottle', 'cable')\n",
        "            category_path = os.path.join(root_dir, category, self.data_type)  # Choose Train or Val\n",
        "\n",
        "            gt_clean_image_path = os.path.join(category_path, 'GT_clean_image')\n",
        "            degraded_image_path = os.path.join(category_path, 'Degraded_image')\n",
        "\n",
        "            # Subfolders within 'GT_clean_image' and 'Degraded_image' (e.g., 'broken_large', 'broken_small')\n",
        "            for subfolder in os.listdir(gt_clean_image_path):\n",
        "                clean_subfolder = os.path.join(gt_clean_image_path, subfolder)\n",
        "                degraded_subfolder = os.path.join(degraded_image_path, subfolder)\n",
        "\n",
        "                # For each image in the subfolder, gather the corresponding file paths\n",
        "                for img_name in os.listdir(clean_subfolder):\n",
        "                    clean_img = os.path.join(clean_subfolder, img_name)\n",
        "                    degraded_img = os.path.join(degraded_subfolder, img_name)\n",
        "\n",
        "                    # Append to lists\n",
        "                    self.clean_images.append(clean_img)\n",
        "                    self.degraded_images.append(degraded_img)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_img = Image.open(self.clean_images[idx]).convert('RGB')\n",
        "        degraded_img = Image.open(self.degraded_images[idx]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            clean_img = self.transform(clean_img)\n",
        "            degraded_img = self.transform(degraded_img)\n",
        "\n",
        "        return degraded_img, clean_img # Return only clean and degraded images\n",
        "\n",
        "# Step 4: Define transformations (resizing to a uniform size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),  # Resize all images to the same size\n",
        "    transforms.ToTensor()           # Convert images to PyTorch tensors\n",
        "])          # Convert images to PyTorch tensors\n",
        "\n",
        "# Step 5: Load training and validation datasets\n",
        "data_path = os.path.join(extract_path, 'Denoising_Dataset_train_val')\n",
        "train_dataset = DenoisingDataset(root_dir=data_path, data_type='Train', transform=transform)\n",
        "val_dataset = DenoisingDataset(root_dir=data_path, data_type='Val', transform=transform)\n",
        "\n",
        "print(f\"Number of images in the training set: {len(train_dataset)}\")\n",
        "print(f\"Number of images in the validation set: {len(val_dataset)}\")\n",
        "\n",
        "# Step 6: Create DataLoader for training and validation datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btuM0INYXLLA"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn9DQtqvWQUL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Proposed_U-net\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        residual = self.residual_conv(x) if self.residual_conv else x\n",
        "        return out + residual\n",
        "\n",
        "class SCA(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SCA, self).__init__()\n",
        "        self.spatial_attention = nn.Conv2d(in_channels, in_channels, kernel_size=1)  # Pointwise Convolution for Channel Attention\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels // 16, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // 16, in_channels, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Spatial Attention\n",
        "        spatial_out = self.spatial_attention(x)\n",
        "        spatial_out = torch.sigmoid(spatial_out)\n",
        "\n",
        "        # Channel Attention\n",
        "        channel_out = self.channel_attention(x)\n",
        "\n",
        "        # Element-wise multiplication for attention fusion\n",
        "        fused_out = x * spatial_out * channel_out\n",
        "\n",
        "        return fused_out\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.theta = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1)\n",
        "        self.phi = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1)\n",
        "        self.g = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1)\n",
        "        self.W = nn.Conv2d(in_channels // 2, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "\n",
        "        # Compute theta, phi, g\n",
        "        theta_x = self.theta(x).view(batch_size, C // 2, -1)\n",
        "        phi_x = self.phi(x).view(batch_size, C // 2, -1)\n",
        "        g_x = self.g(x).view(batch_size, C // 2, -1)\n",
        "\n",
        "        # Compute attention map\n",
        "        f = torch.matmul(theta_x.permute(0, 2, 1), phi_x)  # (B, H*W, H*W)\n",
        "        f_div_C = f / (H * W)  # Normalize\n",
        "        y = torch.matmul(f_div_C, g_x.permute(0, 2, 1))  # (B, H*W, C//2)\n",
        "        y = y.view(batch_size, C // 2, H, W)  # Reshape\n",
        "\n",
        "        # Combine with original features\n",
        "        y = self.W(y)\n",
        "        return x + y  # Residual connection\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dconv_down1 = double_conv(3, 16)\n",
        "        self.att1 = SCA(16)\n",
        "\n",
        "        self.dconv_down2 = double_conv(16, 32)\n",
        "        self.att2 = SCA(32)\n",
        "\n",
        "        self.dconv_down3 = double_conv(32, 64)\n",
        "        self.att3 = SCA(64)\n",
        "\n",
        "        self.dconv_down4 = double_conv(64, 128)\n",
        "        self.att4 = SCA(128)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.dconv_bottleneck = double_conv(128, 128)  # Bottleneck convolution\n",
        "        self.nl_block = NonLocalBlock(128)  # Non-Local Block\n",
        "\n",
        "        self.dconv_up3 = double_conv(64 + 128, 64)\n",
        "        self.dconv_up2 = double_conv(32 + 64, 32)\n",
        "        self.dconv_up1 = double_conv(16 + 32, 16)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(16, n_class, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_input = x\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        conv1s = self.att1(conv1)  # Apply attention to skip connection\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        conv2s = self.att2(conv2)  # Apply attention to skip connection\n",
        "        x = self.maxpool(conv2)\n",
        "\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        conv3s = self.att3(conv3)  # Apply attention to skip connection\n",
        "        x = self.maxpool(conv3)\n",
        "\n",
        "        x = self.dconv_down4(x)\n",
        "        x = self.att4(x)  # Apply attention to the bottleneck features\n",
        "\n",
        "        # Bottleneck with Non-Local Block\n",
        "        x = self.dconv_bottleneck(x)\n",
        "        x = self.nl_block(x)  # Apply Non-Local Block\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv3s], dim=1)\n",
        "        x = self.dconv_up3(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2s], dim=1)\n",
        "        x = self.dconv_up2(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1s], dim=1)\n",
        "        x = self.dconv_up1(x)\n",
        "\n",
        "        out = self.conv_last(x)\n",
        "        out = out + x_input\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "n_classes = 3\n",
        "model = UNet(n_class=n_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Move the model to GPU if available\n",
        "\n",
        "# Example input (batch size of 1, 3 channels, 256x256 image)\n",
        "x = torch.randn(1, 3, 256, 256).to(device)\n",
        "output = model(x)\n",
        "print(output.shape)  # Should output: torch.Size([1, 3, 256, 256])\n",
        "\n",
        "# Calculate the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64HnF4hFXrA_"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQhdzG5_Xsxj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
        "\n",
        "class PSNRLoss(nn.Module):\n",
        "    \"\"\"PSNR Loss based on Peak Signal-to-Noise Ratio.\"\"\"\n",
        "\n",
        "    def __init__(self, data_range=1.0):\n",
        "        super(PSNRLoss, self).__init__()\n",
        "        self.psnr = PeakSignalNoiseRatio(data_range=data_range)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Ensure inputs are on the same device\n",
        "        x = torch.tensor(x, device=y.device) if isinstance(x, list) else x\n",
        "        y = torch.tensor(y, device=x.device) if isinstance(y, list) else y\n",
        "\n",
        "        # Compute PSNR\n",
        "        psnr_value = self.psnr(x, y)\n",
        "\n",
        "        # Convert PSNR metric to loss\n",
        "        loss = -psnr_value  # Negate because higher PSNR is better\n",
        "        return loss\n",
        "\n",
        "class SSIMLoss(nn.Module):\n",
        "    \"\"\"SSIM Loss based on Structural Similarity Index Measure.\"\"\"\n",
        "\n",
        "    def __init__(self, data_range=1.0):\n",
        "        super(SSIMLoss, self).__init__()\n",
        "        self.ssim = StructuralSimilarityIndexMeasure(data_range=data_range)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"Calculate SSIM loss between two images.\"\"\"\n",
        "        # Ensure inputs are on the same device\n",
        "        x = torch.tensor(x, device=y.device) if isinstance(x, list) else x\n",
        "        y = torch.tensor(y, device=x.device) if isinstance(y, list) else y\n",
        "\n",
        "        # Compute SSIM\n",
        "        ssim_value = self.ssim(x, y)\n",
        "\n",
        "        # Convert SSIM metric to loss\n",
        "        loss = 1 - ssim_value  # Negate because higher SSIM is better\n",
        "        return loss\n",
        "\n",
        "# Example of usage:\n",
        "# ssim_loss = SSIMLoss()\n",
        "# loss_value = ssim_loss(image1, image2)\n",
        "\n",
        "\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        '''if isinstance(x, list):\n",
        "            x = torch.tensor(x, device=y.device)\n",
        "        else:\n",
        "            x = x.to(y.device)\n",
        "\n",
        "        if isinstance(y, list):\n",
        "            y = torch.tensor(y, device=x.device)\n",
        "        else:\n",
        "            y = y.to(x.device)'''\n",
        "\n",
        "        x = torch.tensor(x, device=y.device) if isinstance(x, list) else x\n",
        "        y = torch.tensor(y, device=x.device) if isinstance(y, list) else y\n",
        "        diff = x - y\n",
        "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps * self.eps)))\n",
        "        return loss\n",
        "\n",
        "class EdgeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EdgeLoss, self).__init__()\n",
        "        k = torch.Tensor([[.05, .25, .4, .25, .05]])\n",
        "        self.kernel = torch.matmul(k.t(), k).unsqueeze(0).repeat(3, 1, 1, 1)\n",
        "        if torch.cuda.is_available():\n",
        "            self.kernel = self.kernel.cuda()\n",
        "        self.loss = CharbonnierLoss()\n",
        "\n",
        "    def conv_gauss(self, img):\n",
        "        n_channels, _, kw, kh = self.kernel.shape\n",
        "        img = F.pad(img, (kw // 2, kh // 2, kw // 2, kh // 2), mode='replicate')\n",
        "        return F.conv2d(img, self.kernel, groups=n_channels)\n",
        "\n",
        "    def laplacian_kernel(self, current):\n",
        "        # Ensure current has four dimensions: [batch, channels, height, width]\n",
        "        if current.dim() == 3:\n",
        "            current = current.unsqueeze(0)\n",
        "\n",
        "        filtered = self.conv_gauss(current)  # Apply Gaussian filter\n",
        "\n",
        "        # Perform downsampling and upsampling with careful indexing\n",
        "        down = filtered[:, :, ::2, ::2]  # Downsample\n",
        "        new_filter = torch.zeros_like(filtered)\n",
        "        new_filter[:, :, ::2, ::2] = down * 4  # Upsample with zero padding\n",
        "\n",
        "        filtered = self.conv_gauss(new_filter)  # Apply Gaussian filter again\n",
        "        diff = current - filtered\n",
        "        return diff\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        loss = self.loss(self.laplacian_kernel(x), self.laplacian_kernel(y))\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpmLp0nSX86j"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5kUYOwJXzCn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # For progress bars\n",
        "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure  # Import metrics\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize your model, loss function, optimizer, and metrics\n",
        "model = UNet(n_class=n_classes).to(device)  # Ensure the model is on the same device\n",
        "criterion_char = CharbonnierLoss().to(device)  # Ensure criterion is on the same device if necessary\n",
        "criterion_ssim = SSIMLoss().to(device)\n",
        "criterion_edge = EdgeLoss().to(device)  # Ensure criterion is on the same device if necessary\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize metrics for PSNR and SSIM\n",
        "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
        "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "\n",
        "# Define the number of epochs and paths for saving the model\n",
        "num_epochs = 250\n",
        "best_ssim = 0.0\n",
        "best_model_path = 'best_model.pth'\n",
        "\n",
        "# Lists to store the training and validation losses and metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_psnr_scores = []\n",
        "train_ssim_scores = []\n",
        "val_psnr_scores = []\n",
        "val_ssim_scores = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_train_loss = 0.0\n",
        "    running_train_psnr = 0.0\n",
        "    running_train_ssim = 0.0\n",
        "\n",
        "    # Training phase\n",
        "    for degraded_imgs, clean_imgs in tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
        "        # Move images to the device\n",
        "        degraded_imgs, clean_imgs = degraded_imgs.cuda(), clean_imgs.cuda()\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(degraded_imgs)  # Forward pass\n",
        "\n",
        "\n",
        "        # Calculate losses\n",
        "        ''''loss_char = torch.sum(torch.stack([criterion_char(outputs[j], clean_imgs) for j in range(len(outputs))]))\n",
        "        loss_edge = torch.sum(torch.stack([criterion_edge(outputs[j], clean_imgs) for j in range(len(outputs))]))\n",
        "        loss_ssim = torch.sum(torch.stack([criterion_ssim(outputs[j], clean_imgs) for j in range(len(outputs))]))  # Negate PSNR\n",
        "        loss = loss_char + (0.1 * loss_edge) + (0.3*loss_ssim)'''\n",
        "        loss_char = criterion_char(outputs, clean_imgs)  # Outputs shape: [batch_size, channels, height, width]\n",
        "        loss_edge = criterion_edge(outputs, clean_imgs)\n",
        "        loss_ssim = criterion_ssim(outputs, clean_imgs)\n",
        "        loss = loss_char + (0.1 * loss_edge) + (0.3*loss_ssim)\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        psnr = psnr_metric(outputs, clean_imgs)  # Use the final output for PSNR and SSIM\n",
        "        ssim = ssim_metric(outputs, clean_imgs)\n",
        "        running_train_psnr += psnr.item()\n",
        "        running_train_ssim += ssim.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    avg_train_psnr = running_train_psnr / len(train_loader)\n",
        "    avg_train_ssim = running_train_ssim / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_psnr_scores.append(avg_train_psnr)\n",
        "    train_ssim_scores.append(avg_train_ssim)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_val_loss = 0.0\n",
        "    running_val_psnr = 0.0\n",
        "    running_val_ssim = 0.0\n",
        "\n",
        "    with torch.no_grad():  # No gradients required during validation\n",
        "        for degraded_imgs, clean_imgs in tqdm(val_loader, desc=f'Validating Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
        "            degraded_imgs, clean_imgs = degraded_imgs.to(device), clean_imgs.to(device)\n",
        "            outputs = model(degraded_imgs)\n",
        "\n",
        "            # Calculate losses\n",
        "            loss_char = criterion_char(outputs, clean_imgs)  # Outputs shape: [batch_size, channels, height, width]\n",
        "            loss_edge = criterion_edge(outputs, clean_imgs)\n",
        "            loss_ssim = criterion_ssim(outputs, clean_imgs)\n",
        "            loss = loss_char + (0.1 * loss_edge) + (0.3*loss_ssim)\n",
        "            #loss = loss_char + (0.05 * loss_edge)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr = psnr_metric(outputs, clean_imgs)\n",
        "            ssim = ssim_metric(outputs, clean_imgs)\n",
        "            running_val_psnr += psnr.item()\n",
        "            running_val_ssim += ssim.item()\n",
        "\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    avg_val_psnr = running_val_psnr / len(val_loader)\n",
        "    avg_val_ssim = running_val_ssim / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_psnr_scores.append(avg_val_psnr)\n",
        "    val_ssim_scores.append(avg_val_ssim)\n",
        "\n",
        "    # Print the average losses and metrics for this epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Validation Loss: {avg_val_loss:.4f}, Train PSNR: {avg_train_psnr:.4f}, \"\n",
        "          f\"Validation PSNR: {avg_val_psnr:.4f}, Train SSIM: {avg_train_ssim:.4f}, \"\n",
        "          f\"Validation SSIM: {avg_val_ssim:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if avg_val_ssim > best_ssim:\n",
        "        best_ssim = avg_val_ssim\n",
        "        torch.save(model.state_dict(), best_model_path)  # Save the model\n",
        "\n",
        "# Plotting the training and validation losses and metrics\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', color='orange')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), train_psnr_scores, label='Training PSNR', color='blue')\n",
        "plt.plot(range(1, num_epochs + 1), val_psnr_scores, label='Validation PSNR', color='orange')\n",
        "plt.plot(range(1, num_epochs + 1), train_ssim_scores, label='Training SSIM', color='green')\n",
        "plt.plot(range(1, num_epochs + 1), val_ssim_scores, label='Validation SSIM', color='red')\n",
        "plt.title('Training and Validation PSNR/SSIM over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_validation_metrics_curve.png')  # Save the figure\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
